import Code from '../../components/common/Code';

export const metadata = {
  title: 'On Developer Experience',
  alternates: {
    canonical: '/n/dx',
  },
};

## First

sandes thapa is dangerouslySetInnerHTML dfdsf dsf
The AI model generates several possible responses to a given input or question.
Human reviewers compare these responses and rank them from best to worst based on criteria like accuracy, helpfulness, and alignment.
Using the human rankings, a separate model called a "reward model" is trained to predict the quality of responses.
The original AI model is then fine-tuned using reinforcement learning techniques guided by the reward model.
For example, if you want a language model for customer support, you might fine-tune it on transcripts of customer service interactions and then use RLHF to ensure it responds politely and effectively resolves customer issues. Increasingly, AI is used to generate initial content and humans help refine it through this feedback loop, enhancing the model's performance in real-world applications.

.



This is text about developer experience. This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.This is text about developer experience.

- `NEXT_PUBLIC_SITE_URL`: The URL of your website.

## next heading


Critical Caveat: Layout authorization is a convenience layer, NOT a security solution. The true, comprehensive authorization must occur in API, Service, and/or Data Access Layers.

Strategy 1: Use Layout checks as a DX improvement, but implement robust, independent authorization checks in backend layers to ensure profound security.

Strategy 2: Implement authorization checks in Page components to ensure security, even if it means sacrificing developer experience and convenience.

Strategy 3: Combine both strategies to achieve a balance between security and developer experience (which may be redundant though).

In a growing application, when using Layout components as first line of defense for authorization, you will eventually implement more Layout components for authorization when roles and permissions come into play.

For example, you might have an AdminLayout component that checks if the user is an admin before rendering the children:


## sandseh is testing again 
Personal computers became mainstream in the 90s. Yet in a strange twist, the software they ended up running wasn’t very personal at all.

Sure, you could bring the machine home, plug it in, and have your very own computer. But the programs inside—the operating systems, the massive office suites—were built for everyone. It was one-size-fits-all software.

Of course, the power users could hack and customize their machine. But most computer users were stuck using software stuffed with features they didn’t need.

If you wanted to do one small thing—say, convert text from one format to another—you were forced to wade through hundreds of menus and options irrelevant to your particular need. Ironically, this was personal computing without the personal.

## Single-use software
AI has changed our relationship with software.

Software can now adapt to you, not the other way around. Better yet, AI is making it possible for anyone, not just developers, to create single-use or custom applications.

Think about it: if you’re a typical computer user, there used to be a big hurdle to “building software.” You had to learn a programming language, deal with complex tools, figure out how to actually get the site online. Even worse, dealing with mobile app stores.

We’re now seeing a world where you can build your ideas. Sometimes these apps, which precisely solve your own problems, are easier to create than anything you could purchase off the shelf.

For example, I recently became a father. My wife and I wanted to track how the baby was sleeping and eating. We didn’t need user profiles, badges, subscription tiers, or any other extra features. Why not build our own personal software?

This might start with developers. But within the next decade, millions of people will be able to create software. Designers, marketers, product mangers, and others will be the first. They will be able to build their own ideas.

## Home-cooked applications
Creating software is starting to resemble cooking.

If you want a meal just for yourself—maybe you prefer your eggs a particular way—you can do that without needing a professional chef. The results might not compare to a top restaurant, but that’s not the point.

Your home-cooked software is exactly what you need, without extra fuss or cost. And each time you build something personal, you gain a deeper appreciation for the craft of creating software.

Does that mean frontend development or design is dead? No, not at all.

The more that build software, the more who will appreciate using great software. Cooking at home makes you appreciate going out to eat.

### The personal software era
What if making single-use apps were 10x easier than today? 100x easier?

You wouldn’t search “best chrome extensions for note taking”. You would work with AI. In five minutes, you’d have something that works exactly how you want.

As tools improve and become more ubiquitous, everyone can cook. And once you start cooking, you might still dine out—perhaps even more—because now you truly appreciate how good a well-crafted meal can be.

We need more builders, not fewer. Because building fosters understanding. And as more people start making personal software, the bar for what counts as “great software” will inevitably rise.

We’ll see more innovations, more ideas, more glimpses of what’s possible when individuals have the freedom to solve their own problems. And if you’re just hacking something together for yourself, don’t worry if it’s not perfect.

That’s part of the fun. After all, it’s personal.

Understanding AI
Lee Robinson
I wanted to better understand how AI models are created. Not to become an expert, but to gain an appreciation for the abstractions I use every day.

This post will highlight what I’ve learned so far. It’s written for other engineers who are new to topics like neural networks, deep learning, and transformers.

Machine Learning
Software is deterministic. Given some input, if you run the program again, you will get the same output. A developer has explicitly written code to handle each case.

Most AI models¹ are not this way — they are probabilistic. Developers don’t have to explicitly program the instructions.

Machine learning teaches software to recognize patterns from data. Given some input, you might not get the same output². AI models like GPT (from OpenAI), Claude (from Anthropic), and Gemini (from Google) are “trained” on a large chunk of internet documents. These models learn patterns during training.

Then, there’s an API or chat interface where you can talk to the model. Based on some input, it can predict and generate sentences, images, or audio as output. You can think about machine learning as a subset of the broader AI category.

Neural Networks
AI models are built on neural networks — think of them as a giant web of decision-making pathways that learn from examples. Neural networks can be used for anything, but I'll focus on language models.

San
Francisco (95%)
Output: San

0.5x
1x
1.5x
2x
These networks consist of layers of interconnected neurons that process information:

An input layer where data enters the system. Input is converted into a numerical representation of words or tokens (more on tokens later).
Many hidden layers that create an understanding of patterns in the system. Neurons inside the layer apply weights (also known as parameters) to the input data and pass the result through an activation function³. This function outputs a value, often between 0 and 1, representing the neuron's level of activation.
An output layer which produces the final result, such as predicting the next word in a sentence. The outputs at this stage are often referred to as logits, which are raw scores that get transformed into probabilities.
For example, if the input was “San”, there is likely an activation close to 1 for the next word of “Francisco”. A unrelated word like “kitten” would be close to 0.

A big takeaway for me is: it’s just math. You can build a neural network from first principles using linear algebra, calculus, and statistics. You likely won’t do this when there’s helpful abstractions like PyTorch, but it’s helpful for me to demystify what is happening under the hood.

Deep Learning


## Pretraining
Large Language Models (LLMs) are trained on vast amounts of data.

Collecting and cleaning this data is much more complex than simply scraping websites. If you feed the model poor-quality data, you'll get poor-quality predictions in return. Garbage in, garbage out.

Training, also called pretraining, is the process of teaching the network to recognize patterns in the input data. Essentially, we're taking large chunks of internet text and compressing them into knobs the model can tune, known as weights and biases (collectively called parameters).

The number of parameters is often used as a measure of a model's power. For example, open-source models like Llama have a 7 billion and 70 billion parameter version (with the latter being more powerful).

You can think of training like a big for loop that runs many times, each time adjusting the weights and biases slightly. Each complete pass through the entire training dataset is called an epoch. But how does the network know how to adjust these values?

The training process involves several key steps:

Forward Pass: The network takes input data and passes it through its layers to produce an output. This is called the forward pass. During this step, the network makes a prediction based on its current weights and biases.
Loss Function: After obtaining the output, we need to measure how good or bad the prediction is. This is done using a loss function, which measures the prediction and gives us a value to try and minimize.
Backward Pass (Backpropagation): To improve the network's performance, we need to adjust the weights and biases to reduce the loss. This involves computing the gradient of the loss function with respect to each weight and bias. The gradients indicate the direction and magnitude by which to adjust the parameters to minimize the loss.
Gradient Descent: Using the calculated gradients, we update the weights and biases. This process is called gradient descent, an optimization algorithm⁴ that iteratively adjusts the parameters to minimize the loss function.
Training Pseudocode
By repeatedly performing these steps over many epochs, the network learns to make better predictions. It uses the gradients calculated during backpropagation to adjust its internal parameters, effectively minimizing the loss function and improving its performance over time.

## Aside: What are scaling laws?
Transformers
One paper changed the course of AI: "Attention Is All You Need". It introduced the transformer architecture, a more efficient and powerful way of building language models.



## Fine-tuning
After pretraining, the model may need to be adapted for specific tasks or aligned with human values. Fine-tuning is the process of adjusting a pretrained model to have a unique style or perform well in a specific domain.

Fine-tuning involves using a smaller, high-quality dataset relevant to the specific task. This data is often human-reviewed to ensure quality and alignment with the desired outcomes. During fine-tuning, the model's parameters—the weights and biases we've discussed earlier—are adjusted slightly to improve performance on this new dataset.

To further improve the model's usefulness and ensure it behaves in desirable ways, a technique called Reinforcement Learning from Human Feedback (RLHF) is used. RLHF involves a few steps:

The AI model generates several possible responses to a given input or question.
Human reviewers compare these responses and rank them from best to worst based on criteria like accuracy, helpfulness, and alignment.
Using the human rankings, a separate model called a "reward model" is trained to predict the quality of responses.
The original AI model is then fine-tuned using reinforcement learning techniques guided by the reward model.
For example, if you want a language model for customer support, you might fine-tune it on transcripts of customer service interactions and then use RLHF to ensure it responds politely and effectively resolves customer issues. Increasingly, AI is used to generate initial content and humans help refine it through this feedback loop, enhancing the model's performance in real-world applications.

Inference
After training and fine-tuning, the model is ready for inference—the process of making predictions or generating outputs from new inputs. During inference, the model applies what it has learned to produce responses. The model may support multimodal input (text, images, audio, etc) and output.

Modern language models can perform complex reasoning tasks using techniques like chain of thought. This involves the model generating intermediate reasoning steps before arriving at a final answer, much like how you might work through a math problem step by step.


